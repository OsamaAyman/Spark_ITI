{
 "cells": [
  {
   "cell_type": "raw",
   "id": "612707a4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "419f591c",
   "metadata": {},
   "source": [
    "# Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853b5cef",
   "metadata": {},
   "source": [
    "#### Application Scenario \n",
    "\n",
    "* Imagine that you have a simple social network or ecommerce application which consists of an interactive UI, a web server, and a database. \n",
    "* you need to keep records of events such as clicks, requests, impressions and searches that take place on your web application and store them for computation, reporting, and analysis, each of which is done by separate applications or services.\n",
    "* solution: to store all of events in any kind of databases and develop services to deal with the data stored in db\n",
    "* but you find several problems related to processing, storing and data formats.\n",
    "\n",
    "#### you need streaming platform\n",
    "\n",
    "that can do the following:\n",
    "\n",
    "    1. Store a huge amount of data that can be persistent, checksummed and replicated for fault tolerance\n",
    "    2. Process continuous flow of data (data streams) in real time across systems\n",
    "    3. Allow applications to publish data or data streams independently and agnostic to the application/service consuming it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adba720e",
   "metadata": {},
   "source": [
    "Kafka is a distributed platform and built for scale, which means it can handle sky-high frequency reads and writes & store huge volumes of data. \n",
    "\n",
    "It ensures that the data is always reliable. \n",
    "\n",
    "It also supports strong mechanisms for recovery from failures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c00795",
   "metadata": {},
   "source": [
    "1. simplify the backend arch.\n",
    "2. universal pipeline of data\n",
    "    * Any application can push data to this platform which can later be pulled by another application.\n",
    "    * Kafka makes it easy to exchange data between applications\n",
    "3. connects to existing systems\n",
    "    * offers a framework called Kafka Connect to connect to legacy systems\n",
    "4. process data in realtime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77031802",
   "metadata": {},
   "source": [
    "## what's kafka\n",
    "\n",
    "1. runs on cluster of 1 or more servers (brokers)\n",
    "2. streams of records are stored in **topics**.\n",
    "    * each record consists of *key, value and timestamp*\n",
    "3. publish-subscribe pattern\n",
    "    * **producers** apps publish to topics\n",
    "    * **consumers** apps subscribe to topics\n",
    "4. APIs (consumer, producer, connector)for existing systems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6767e91",
   "metadata": {},
   "source": [
    "**topics** are divided into partitions, and replicated across broker servers of the cluster, so we can use parallel access to data.\n",
    "\n",
    "each **broker** has several **partitions** that are either **leader** or **replica**, if the process fails then the system check the data from the replicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ceeba077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kafka-python in /home/mohamed/anaconda3/lib/python3.8/site-packages (2.0.2)\n",
      "Collecting kazoo\n",
      "  Downloading kazoo-2.8.0-py2.py3-none-any.whl (142 kB)\n",
      "\u001b[K     |████████████████████████████████| 142 kB 199 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /home/mohamed/.local/lib/python3.8/site-packages (from kazoo) (1.16.0)\n",
      "Installing collected packages: kazoo\n",
      "Successfully installed kazoo-2.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install kafka-python\n",
    "!pip install kazoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec569755",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '<port>'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_155068/314694758.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkafka\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKafkaProducer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m producer = KafkaProducer(\n\u001b[0m\u001b[1;32m      4\u001b[0m  \u001b[0mbootstrap_servers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'<host>:<port>'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m  \u001b[0msecurity_protocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"SSL\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/kafka/producer/kafka.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **configs)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreporters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         client = KafkaClient(metrics=self._metrics, metric_group_prefix='producer',\n\u001b[0m\u001b[1;32m    382\u001b[0m                              \u001b[0mwakeup_timeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_block_ms'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                              **self.config)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/kafka/client_async.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **configs)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'selector'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClusterMetadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# empty set will fetch all topic metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata_refresh_in_progress\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/kafka/cluster.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **configs)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bootstrap_brokers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_bootstrap_brokers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coordinator_brokers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/kafka/cluster.py\u001b[0m in \u001b[0;36m_generate_bootstrap_brokers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_generate_bootstrap_brokers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# collect_hosts does not perform DNS, so we should be fine to re-use\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mbootstrap_hosts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollect_hosts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bootstrap_servers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mbrokers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/kafka/conn.py\u001b[0m in \u001b[0;36mcollect_hosts\u001b[0;34m(hosts, randomize)\u001b[0m\n\u001b[1;32m   1495\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhost_port\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhosts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1497\u001b[0;31m         \u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mafi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ip_port_afi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost_port\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mport\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/kafka/conn.py\u001b[0m in \u001b[0;36mget_ip_port_afi\u001b[0;34m(host_and_port_str)\u001b[0m\n\u001b[1;32m   1476\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1477\u001b[0m             \u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhost_and_port_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m':'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1478\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1480\u001b[0m             \u001b[0maf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_address_family\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '<port>'"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaProducer\n",
    "import json\n",
    "producer = KafkaProducer(\n",
    " bootstrap_servers='<host>:<port>',\n",
    " security_protocol=\"SSL\",\n",
    " ssl_cafile=\"./ca.pem\",\n",
    " ssl_certfile=\"./service.cert\",\n",
    " ssl_keyfile=\"./service.key\",\n",
    " value_serializer=lambda v: json.dumps(v).encode('ascii')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483a0574",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
